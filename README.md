# SenseWAI.us
SenseWAI aims to warn blind pedestrians of incoming objects using the AI powered computer vision capabilities of their phone cameras. Using a custom trained YOLOv11 model, a powerful neural network implemented by Ultralytics, detecting crosswalks and relaying information to the blind user can occur within moments. Because SenseWAI is tailored towards those who are visually impaired, our application is completely functional with minimal UI. However, YOLO highlights the area around identified people, objects, etc. in different colors and serves as a way for people with sight to visualize how important SenseWAI can be for those who have difficulty seeing. Additionally, our extensive prompt engineering of the Google Gemini AI API, coupled with the use of the Speech Synthesis TTS API, returns clear spoken feedback on the user's surroundings based on a real time snapshot from their camera feed. It also gives warnings for obstacles that are in the path or coming towards the user, among other various details.

SenseWAI has a plain mode that doesn't display any visuals or camera feed, but still returns spoken feedback, as well as a visual mode showcasing YOLO's abilities for demo purposes. There are also three different actions a user can take while interacting with SenseWAI. First, the user can tap the screen to receive a singular real time description. Second, the user can hold down on the screen to receive a continuous stream of descriptions. This is ideal for short walks. Third, the user can double tap to toggle a continuous stream of descriptions without having to hold their device in their hands at all time. This can be extremely helpful especially if the user has a walking stick in combination with SenseWAI. This stream is toggled off with a single screen tap.

To be safely guided by SenseWAI, navigate to sensewai.us and allow browser access to your device's camera. ^u^